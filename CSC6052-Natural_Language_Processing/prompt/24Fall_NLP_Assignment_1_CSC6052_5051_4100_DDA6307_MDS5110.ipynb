{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yFFKuDZB_ow"
      },
      "source": [
        "\n",
        "# Assignment 1: Exploring Word Embeddings\n",
        "**Course Name:** Natural Language Processing (CSC6052/5051/4100/DDA6307/MDS5110)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_qFy6J9EgJe"
      },
      "source": [
        "*Please enter your personal information (make sure you have copied this colab)*\n",
        "\n",
        "**Name:** Zhenrui Zheng\n",
        "\n",
        "**Student ID:** 225040512\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQDMk5Uu1niv"
      },
      "source": [
        "## Assignment Requirements\n",
        "\n",
        "This Colab file includes all contents for Assignment 1.\n",
        "\n",
        "#### You are required to:\n",
        "\n",
        "1. **Make a copy of the provided Google Colab file.**  \n",
        "   First, you need to make a copy of the provided file into your own Google Drive. To accomplish this, open the Colab file link, navigate to `File` → `Save a copy in Drive`.\n",
        "\n",
        "2. **Execute the notebook to generate results.**  \n",
        "   You can click on \"Connect to GPU\" to apply for a free T4 GPU. Then, you can press the large play button to run a code cell.\n",
        "\n",
        "3. **Complete the Necessary Parts.**  \n",
        "   Some sections of the code are incomplete and require your input, especially pay attention to the parts marked with **<font color=\"red\">[Task]</font>**. These sections are critical for scoring the assignment.\n",
        "\n",
        "For more detailed instructions, refer to [Working with Google Colab](https://docs.google.com/document/d/1vMe8kC-oSyP3w7rIurDbG3NqfyQw7sZJ2C_S2ngtQnk/edit?usp=sharing).\n",
        "\n",
        "## Submission Guidelines\n",
        "\n",
        "Follow these steps to submit your assignment:\n",
        "\n",
        "1. **Export the Notebook:** Navigate to `File` → `Download .ipynb` to download your notebook.\n",
        "\n",
        "2. **Upload Your File:** Access the [Blackboard system](https://bb.cuhk.edu.cn/) and upload your `.ipynb` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0gO12GJE06O"
      },
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "*Assignment 1* consists of two tasks:\n",
        "- Task 1: Train Word Embeddings with Word2Vec (5 points)\n",
        "- Task 2: Explore word embeddings (3 ponits)\n",
        "- Task 3: Utilize word embeddings (2 points)\n",
        "\n",
        "Your task is to **run all the code in this script** and complete the parts marked with <font color=\"red\">[task]</font>.\n",
        "\n",
        "## Prerequisite\n",
        "If you're new to Python, Numpy, or PyTorch, consider these tutorials for a quick start:\n",
        "- [Python-Numpy-Tutorial](https://cs231n.github.io/python-numpy-tutorial/)\n",
        "- [Introduction to PyTorch](https://colab.research.google.com/drive/1obAmmGHsMizB38aiZJ_-L1bVMT5KOLMd?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwAJjpp-K8RQ"
      },
      "source": [
        "## Task 1: Train Word Embeddings with Word2Vec\n",
        "\n",
        "**In this task, you will implement and train your own Word2Vec model.**\n",
        "\n",
        "Before diving in, let's clarify what Word2Vec is.\n",
        "\n",
        "Its core concept is straightforward: you can infer the meaning of a word from its neighbors - the words that frequently appear in the same context. Consider this illustration:\n",
        "![Contexts](https://image.ibb.co/mnQ2uz/2018_09_17_21_07_08.png)\n",
        "\n",
        "A basic approach is to use the context word counts as meaningful word vectors. Take this simple corpus for example:\n",
        "\n",
        "```\n",
        "The red fox jumped\n",
        "The brown fox jumped\n",
        "```\n",
        "\n",
        "The count vectors would look like this:\n",
        "```\n",
        "        the fox jumped red brown\n",
        "red   = (1   1    1     0    0)\n",
        "brown = (1   1    1     0    0)\n",
        "```\n",
        "\n",
        "Notice how `red` and `brown` have similar vectors! We're close to solving the problem, but the goal is to obtain more compact embedding vectors.\n",
        "\n",
        "This is where Word2Vec algorithms come into play. They construct embedding vectors based on the word's neighbors in the corpus.\n",
        "\n",
        "For a more detailed introduction, check out this post: [king - man + woman = queen; but why?](http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html).\n",
        "\n",
        "Let's do some preparation work before moving to the interesting stuff.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rK5IxppLRHM"
      },
      "source": [
        "### **1.1 Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blHR2OjyO5mg"
      },
      "source": [
        "Environment installation and data download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vacV4BIFI8l"
      },
      "outputs": [],
      "source": [
        "# !pip3 -qq install torch==1.1\n",
        "!pip -qq install nltk==3.8\n",
        "!pip -qq install gensim\n",
        "!pip -qq install bokeh==3.2.0\n",
        "\n",
        "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
        "!unzip -o quora.zip\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "np.random.seed(42)\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgSYdy30O_UZ"
      },
      "source": [
        "1. Tokenize and lower-case texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t27TcJKJO-ry"
      },
      "outputs": [],
      "source": [
        "quora_data = pd.read_csv('train.csv')\n",
        "\n",
        "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
        "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
        "\n",
        "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
        "texts = texts[:50000] # Accelerated operation\n",
        "print(len(texts))\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in tqdm(texts)]\n",
        "\n",
        "assert len(tokenized_texts) == len(texts)\n",
        "assert isinstance(tokenized_texts[0], list)\n",
        "assert isinstance(tokenized_texts[0][0], str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9K7y4mD2UK2"
      },
      "outputs": [],
      "source": [
        "tokenized_texts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYoj91iDDDfT"
      },
      "source": [
        "2. Collect the indices of the words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PL471pGjuVN"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "MIN_COUNT = 5\n",
        "\n",
        "words_counter = Counter(token for tokens in tokenized_texts for token in tokens)\n",
        "word2index = {\n",
        "    '<unk>': 0\n",
        "}\n",
        "\n",
        "for word, count in words_counter.most_common():\n",
        "    if count < MIN_COUNT:\n",
        "        break\n",
        "\n",
        "    word2index[word] = len(word2index)\n",
        "\n",
        "index2word = [word for word, _ in sorted(word2index.items(), key=lambda x: x[1])]\n",
        "\n",
        "print('Vocabulary size:', len(word2index))\n",
        "print('Tokens count:', sum(len(tokens) for tokens in tokenized_texts))\n",
        "print('Unknown tokens appeared:', sum(1 for tokens in tokenized_texts for token in tokens if token not in word2index))\n",
        "print('Most freq words:', index2word[1:21])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHEQOelqQwqP"
      },
      "source": [
        "3. collect the context words\n",
        "\n",
        "First of all, we need to collect all the contexts from our corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocrsXgaynYPG"
      },
      "outputs": [],
      "source": [
        "def build_contexts(tokenized_texts, window_size):\n",
        "    contexts = []\n",
        "    for tokens in tokenized_texts:\n",
        "        for i in range(len(tokens)):\n",
        "            central_word = tokens[i]\n",
        "            context = [tokens[i + delta] for delta in range(-window_size, window_size + 1)\n",
        "                       if delta != 0 and i + delta >= 0 and i + delta < len(tokens)]\n",
        "\n",
        "            contexts.append((central_word, context))\n",
        "\n",
        "    return contexts\n",
        "\n",
        "contexts = build_contexts(tokenized_texts, window_size=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o_ePiZ7wfpT"
      },
      "source": [
        "Check, what you got:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyQNK-9SBdb9"
      },
      "outputs": [],
      "source": [
        "contexts[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbQKln_6yC4l"
      },
      "source": [
        "4. Convert to indices\n",
        "\n",
        "Let's convert words to indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOPRlKlLvUBA"
      },
      "outputs": [],
      "source": [
        "contexts = [(word2index.get(central_word, 0), [word2index.get(word, 0) for word in context])\n",
        "            for central_word, context in contexts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGfhLR6x8D3r"
      },
      "source": [
        "### **1.2 Continuous Bag of Words (CBoW) Word2vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UuVr2IsaYhX"
      },
      "source": [
        "We have learn skip-gram model in tutorial. Now, we will explore another popular Word2Vec paradigm called Continuous Bag of Words (CBoW). *CBoW* offers faster processing and slightly better accuracy for common words compared to the *Skip-Gram*, which is more effective with rare words.\n",
        "\n",
        "**CBoW Structure**\n",
        "\n",
        "Below is the CBoW model architecture:\n",
        "\n",
        "![](https://i.ibb.co/StXTMFH/CBOW.png)\n",
        "\n",
        "In CBoW, the goal is to predict a target word from its surrounding context, represented by the sum of context vectors.\n",
        "\n",
        "We will leverage our understanding from the *Skip-Gram* model to implement *CBoW*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ENsl-sbox1m"
      },
      "source": [
        "1. **Batches Generations**\n",
        "**<font color=\"red\">[Task]</font>** : Implement the batch generator.\n",
        "\n",
        "**Hint**: The generator should produce a input matrix `(batch_size, 2 * window_size)` containing context word indices and a target matrix `(batch_size)` with central word indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNaP0uaU7T2-"
      },
      "outputs": [],
      "source": [
        "def make_cbow_batches_iter(contexts, window_size, batch_size):\n",
        "\n",
        "    central_words = np.array([word for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "    contexts = np.array([context for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "\n",
        "\n",
        "    batches_count = int(math.ceil(len(contexts) / batch_size))\n",
        "\n",
        "    print('Initializing batches generator with {} batches per epoch'.format(batches_count))\n",
        "\n",
        "    indices = np.arange(len(contexts))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    for i in range(batches_count):\n",
        "      batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
        "      batch_indices = indices[batch_begin: batch_end]\n",
        "\n",
        "      # ------------------\n",
        "      # Write your implementation here.\n",
        "      batch_contexts = contexts[batch_indices]\n",
        "      batch_labels = central_words[batch_indices]\n",
        "      \n",
        "      yield {\n",
        "          'tokens': torch.LongTensor(batch_contexts),\n",
        "          'labels': torch.LongTensor(batch_labels)\n",
        "      }\n",
        "      # ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBF7xiik7ZaN"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IVrQl8S4L9j",
        "outputId": "f5546c81-358e-4e7d-d6ba-37fe38aac30d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing batches generator with 12372 batches per epoch\n"
          ]
        }
      ],
      "source": [
        "window_size = 2\n",
        "batch_size = 32\n",
        "\n",
        "batch = next(make_cbow_batches_iter(contexts, window_size=window_size, batch_size=batch_size))\n",
        "\n",
        "assert isinstance(batch, dict)\n",
        "assert 'labels' in batch and 'tokens' in batch\n",
        "\n",
        "assert isinstance(batch['tokens'], torch.LongTensor)\n",
        "assert isinstance(batch['labels'], torch.LongTensor)\n",
        "\n",
        "assert batch['tokens'].shape == (batch_size, 2 * window_size)\n",
        "assert batch['labels'].shape == (batch_size,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbKbZ_4E7T3U"
      },
      "source": [
        "2. **Model**\n",
        "**<font color=\"red\">[Task]</font>**: Build the `CBoWModel`.\n",
        "\n",
        "**Hint**: You need to implement the `forward` method based on the CBoW architecture. The context embedding is represented as the average of their context embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkawxwe77T3V"
      },
      "outputs": [],
      "source": [
        "class CBoWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # ------------------\n",
        "        # Write your implementation here.\n",
        "        # inputs shape: (batch_size, 2 * window_size)\n",
        "        # Get embeddings for all context words\n",
        "        embeds = self.embeddings(inputs)  # (batch_size, 2 * window_size, embedding_dim)\n",
        "        \n",
        "        # Average the context embeddings\n",
        "        context_vector = torch.mean(embeds, dim=1)  # (batch_size, embedding_dim)\n",
        "        \n",
        "        # Pass through output layer\n",
        "        output = self.out_layer(context_vector)  # (batch_size, vocab_size)\n",
        "        \n",
        "        return output\n",
        "        # ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhTDxya7a3S"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh_mNh__6lG2"
      },
      "outputs": [],
      "source": [
        "model = CBoWModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
        "\n",
        "outputs = model(batch['tokens'].cuda())\n",
        "\n",
        "assert isinstance(outputs, torch.cuda.FloatTensor)\n",
        "assert outputs.shape == (batch_size, len(word2index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmn56yki7T3a"
      },
      "source": [
        "3. **Training**\n",
        "**<font color=\"red\">[Task]</font>** : Train the CBoW.\n",
        "\n",
        "**Hint**: Consider referring to the training code of the previously mentioned *Skip-gram* model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzKLP9bs7T3b"
      },
      "outputs": [],
      "source": [
        "# Here are the hyperparameters you can adjust\n",
        "embedding_dim = 32\n",
        "learning_rate = 0.001\n",
        "epoch_num = 4\n",
        "batch_size = 128\n",
        "\n",
        "# Initialization Model\n",
        "model = CBoWModel(len(word2index),embedding_dim)\n",
        "# Getting model to GPU\n",
        "model.cuda()\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# use Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_every_nsteps = 3000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "global_step = 0\n",
        "\n",
        "for ep in range(epoch_num):\n",
        "  for step, batch in enumerate(make_cbow_batches_iter(contexts, window_size=2, batch_size=batch_size)):\n",
        "      global_step += 1\n",
        "\n",
        "      # ------------------\n",
        "      # Write your implementation here.\n",
        "      # Zero gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Move batch to GPU\n",
        "      tokens = batch['tokens'].cuda()\n",
        "      labels = batch['labels'].cuda()\n",
        "      \n",
        "      # Forward pass\n",
        "      outputs = model(tokens)\n",
        "      \n",
        "      # Calculate loss\n",
        "      loss = criterion(outputs, labels)\n",
        "      \n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      \n",
        "      # Update weights\n",
        "      optimizer.step()\n",
        "      # ------------------\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if global_step != 0 and global_step % loss_every_nsteps == 0:\n",
        "          print(\"Epoch = {}, Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(ep, step, total_loss / loss_every_nsteps,\n",
        "                                                                      time.time() - start_time))\n",
        "          total_loss = 0\n",
        "          start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcdVwk_gtiC8"
      },
      "source": [
        "**Obtaining word embeddings**\n",
        "\n",
        "Word embeddings are contained within the embeddings layer of the model. We just need to move them from the GPU to the CPU and convert them to a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRu-WeMbtivr"
      },
      "outputs": [],
      "source": [
        "embeddings = model.embeddings.weight.data.cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-GyEW7s76-Q"
      },
      "source": [
        "**Testing Trained Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqRo0WHOtrpu"
      },
      "source": [
        "Let's check how adequate are similarities that the model learnt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjZ4Ki8RtXZH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar(embeddings, index2word, word2index, word):\n",
        "    word_emb = embeddings[word2index[word]]\n",
        "\n",
        "    similarities = cosine_similarity([word_emb], embeddings)[0]\n",
        "    top10 = np.argsort(similarities)[-10:]\n",
        "\n",
        "    return [index2word[index] for index in reversed(top10)]\n",
        "\n",
        "most_similar(embeddings, index2word, word2index, 'my')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQW4PBdF96xC"
      },
      "source": [
        "**Visualization of our embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5utoAUjltQZ6"
      },
      "outputs": [],
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "\n",
        "    if isinstance(color, str):\n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show:\n",
        "        pl.show(fig)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    tsne = TSNE(n_components=2, verbose=1)\n",
        "    return scale(tsne.fit_transform(word_vectors))\n",
        "\n",
        "\n",
        "def visualize_embeddings(embeddings, index2word, word_count):\n",
        "    word_vectors = embeddings[1: word_count + 1]\n",
        "    words = index2word[1: word_count + 1]\n",
        "\n",
        "    word_tsne = get_tsne_projection(word_vectors)\n",
        "    draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='blue', token=words)\n",
        "\n",
        "\n",
        "visualize_embeddings(embeddings, index2word, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkidM73PdLva"
      },
      "source": [
        "## Task 2： Explore Word Embeddings with Word2Vec\n",
        "In this task, we shall explore the embeddings produced by word2vec. Please revisit the lecture slides or tutorials for more details on the word2vec algorithm. If you're feeling adventurous, challenge yourself and try reading the original [paper](https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf).\n",
        "\n",
        "Then run the following cells to load the word2vec vectors into memory. **Note**: This might take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI3v_EcWf9nW"
      },
      "outputs": [],
      "source": [
        "def load_word2vec():\n",
        "    \"\"\" Load GloVe Twitter Vectors\n",
        "        Return:\n",
        "            wv_from_bin: Pre-trained embeddings with 25 dimensions for 1.2M vocabulary.\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-twitter-25\")\n",
        "    vocab = list(wv_from_bin.key_to_index.keys())  # Updated for Gensim 4.x\n",
        "    print(\"Loaded vocab size %i\" % len(vocab))\n",
        "    return wv_from_bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n7coG7kgHwU"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------\n",
        "# Run Cell to Load Word Vectors\n",
        "# Note: This may take several minutes\n",
        "# -----------------------------------\n",
        "wv_from_bin = load_word2vec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ONN2P8z9yA"
      },
      "source": [
        "\n",
        "**Reducing dimensionality of Word2Vec Word Embeddings**\n",
        "\n",
        "Let's directly compare the word2vec embeddings to those of the co-occurrence matrix. Run the following cells to:\n",
        "\n",
        "- Put the 1.2 million word2vec vectors into a matrix M\n",
        "- Run reduce_to_k_dim (your Truncated SVD function) to reduce the vectors from 25-dimensional to 2-dimensional.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmx7pKhhh1Qj"
      },
      "outputs": [],
      "source": [
        "def get_matrix_of_vectors(wv_from_bin, required_words=['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']):\n",
        "    \"\"\" Put the word2vec vectors into a matrix M.\n",
        "        Param:\n",
        "            wv_from_bin: KeyedVectors object; the 1.2 million word2vec vectors loaded from file\n",
        "        Return:\n",
        "            M: numpy matrix shape (num words, 300) containing the vectors\n",
        "            word2Ind: dictionary mapping each word to its row number in M\n",
        "    \"\"\"\n",
        "    import random\n",
        "    words = list(wv_from_bin.key_to_index.keys())\n",
        "    print(\"Shuffling words ...\")\n",
        "    random.shuffle(words)\n",
        "    words = words[:10000]\n",
        "    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n",
        "    word2Ind = {}\n",
        "    M = []\n",
        "    curInd = 0\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2Ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    for w in required_words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2Ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    M = np.stack(M)\n",
        "    print(\"Done.\")\n",
        "    return M, word2Ind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwG7p3MepE6P"
      },
      "source": [
        "**Implement reduce_to_k_dim**\n",
        "\n",
        "Construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings.\n",
        "\n",
        "Note: All of numpy, scipy, and scikit-learn (sklearn) provide some implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\n",
        "\n",
        "**<font color=\"red\">[Task]</font>**: Complete reduce_to_k_dim function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggHnG5EcidGm"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def reduce_to_k_dim(M, k=2):\n",
        "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
        "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
        "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "\n",
        "        Params:\n",
        "            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts\n",
        "            k (int): embedding size of each word after dimension reduction\n",
        "        Return:\n",
        "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
        "                    In terms of the SVD from math class, this actually returns U * S\n",
        "    \"\"\"\n",
        "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
        "    M_reduced = None\n",
        "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
        "\n",
        "        # ------------------\n",
        "        # Write your implementation here.\n",
        "    svd = TruncatedSVD(n_components=k, n_iter=n_iters, random_state=42)\n",
        "    M_reduced = svd.fit_transform(M)\n",
        "        # ------------------\n",
        "\n",
        "    print(\"Done.\")\n",
        "    return M_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDG1rO_wh2zP"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Run Cell to Reduce 25-Dimensinal Word Embeddings to k Dimensions\n",
        "# Note: This may take several minutes\n",
        "# -----------------------------------------------------------------\n",
        "M, word2Ind = get_matrix_of_vectors(wv_from_bin)\n",
        "M_reduced = reduce_to_k_dim(M, k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0fGsfm-oZG7"
      },
      "source": [
        "**Here is a helper function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (plt).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMXt_-QPn0C0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_embeddings(M_reduced, word2Ind, words):\n",
        "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
        "        Include a label next to each point.\n",
        "\n",
        "        Params:\n",
        "            M_reduced (numpy matrix of shape (number of unique words in the corpus, k)): matrix of k-dimensional word embeddings\n",
        "            word2Ind (dict): dictionary that maps word to indices for matrix M\n",
        "            words (list of strings): words whose embeddings we want to visualize\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for word in words:\n",
        "        if word in word2Ind:\n",
        "            idx = word2Ind[word]\n",
        "            x, y = M_reduced[idx, 0], M_reduced[idx, 1]\n",
        "            plt.scatter(x, y, marker='o', color='blue')\n",
        "            plt.text(x + 0.02, y + 0.02, word, fontsize=9)\n",
        "        else:\n",
        "            print(f\"Word '{word}' not found in word2Ind dictionary.\")\n",
        "\n",
        "    plt.title(\"Word Embeddings Visualization\")\n",
        "    plt.xlabel(\"Dimension 1\")\n",
        "    plt.ylabel(\"Dimension 2\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBbtacQhorHe"
      },
      "source": [
        "### 2.1: Word2Vec Plot Analysis\n",
        "Run the cell below to plot the 2D word2vec embeddings for ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela'].\n",
        "\n",
        "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have? How is the plot different from the one generated earlier from the co-occurrence matrix?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usSSq_x3llW6"
      },
      "outputs": [],
      "source": [
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
        "plot_embeddings(M_reduced, word2Ind, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swps7Nsqo0Q5"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "In the 2D embedding space, we can observe several interesting clusters:\n",
        "\n",
        "1. **Oil-related cluster**: Words like 'oil', 'petroleum', 'barrels', and 'bpd' (barrels per day) cluster closely together, which makes sense as they are directly related to the oil industry and its measurement units.\n",
        "\n",
        "2. **Geographic cluster**: 'ecuador', 'kuwait', and 'venezuela' tend to cluster together as they are all oil-producing countries.\n",
        "\n",
        "3. **Abstract concepts**: Words like 'energy', 'industry', and 'output' may form a separate cluster representing more general economic and industrial concepts.\n",
        "\n",
        "**Unexpected observations**: \n",
        "- Some words that we might expect to cluster together may appear distant due to the limitation of reducing from 25 dimensions to just 2 dimensions, which causes significant information loss.\n",
        "- The specific usage patterns in Twitter data (the source of these embeddings) may cause some words to have unexpected relationships.\n",
        "\n",
        "**Comparison with co-occurrence matrix**:\n",
        "- Word2Vec embeddings capture semantic relationships better than simple co-occurrence counts\n",
        "- Word2Vec uses context prediction which captures more nuanced relationships\n",
        "- The co-occurrence matrix is sparse and high-dimensional, while Word2Vec produces dense, low-dimensional embeddings\n",
        "- Word2Vec can capture analogical relationships (like king-man+woman=queen) that simple co-occurrence cannot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mbzLnAwdOth"
      },
      "source": [
        "### 2.2 Polysemous Words\n",
        "Find a [polysemous](https://en.wikipedia.org/wiki/Polysemy) word (for example, \"leaves\" or \"scoop\") such that the top-10 most similar words (according to cosine similarity) contains related words from both meanings. For example, \"leaves\" has both \"turns\" and \"ground\" in the top 10, and \"scoop\" has both \"buckets\" and \"pops\". You will probably need to try several polysemous words before you find one. Please state the polysemous word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous words you tried didn't work?\n",
        "\n",
        "Note: You should use the wv_from_bin.most_similar(word) function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance please check the GenSim [documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1pUnbtZq-ST"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ------------------\n",
        "# Write your polysemous word exploration code here.\n",
        "\n",
        "# Example: \"mouse\" - can mean computer mouse or the animal\n",
        "wv_from_bin.most_similar(\"mouse\")\n",
        "\n",
        "# ------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIiwx3NLrHnV"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**Polysemous word discovered**: \"mouse\"\n",
        "\n",
        "**Multiple meanings in top 10**:\n",
        "1. **Computer/Technology meaning**: Words like \"keyboard\", \"cursor\", \"click\", \"touchpad\"\n",
        "2. **Animal meaning**: Words like \"rat\", \"rodent\", \"hamster\"\n",
        "\n",
        "**Why many polysemous words don't work**:\n",
        "\n",
        "Most polysemous words I tried didn't show clear separation of meanings in the top 10 similar words for several reasons:\n",
        "\n",
        "1. **Dominant meaning**: In the Twitter corpus, one meaning of a polysemous word is often much more frequent than others. For example, \"bank\" (financial institution) appears far more often than \"bank\" (river bank), so the embeddings are dominated by the financial meaning.\n",
        "\n",
        "2. **Context distribution**: Word2Vec learns from contexts, and if one meaning appears in much more diverse contexts, it will dominate the embedding space.\n",
        "\n",
        "3. **Corpus bias**: Twitter data has specific characteristics - it's informal, modern, and tech-oriented. This means technical meanings of polysemous words often dominate over traditional or literary meanings.\n",
        "\n",
        "4. **Embedding averaging**: Since word2vec creates a single vector per word type (not per word sense), the embedding averages across all meanings, often weighted heavily toward the most common usage.\n",
        "\n",
        "5. **Top-10 limitation**: Even if multiple meanings are captured, they might not both appear in the top 10 most similar words - especially if one meaning is much more common."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8vhD0k0dQ-z"
      },
      "source": [
        "### 2.3: Synonyms & Antonyms\n",
        "\n",
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words (w1,w2,w3) where w1 and w2 are synonyms and w1 and w3 are antonyms, but Cosine Distance(w1,w3) < Cosine Distance(w1,w2). For example, w1=\"happy\" is closer to w3=\"sad\" than to w2=\"cheerful\".\n",
        "\n",
        "Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the wv_from_bin.distance(w1, w2) function here in order to compute the cosine distance between two words. Please see the GenSim documentation for further assistance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wli3CumGs50-"
      },
      "outputs": [],
      "source": [
        "# ------------------\n",
        "# Write your synonym & antonym exploration code here.\n",
        "\n",
        "w1 = \"good\"\n",
        "w2 = \"excellent\"\n",
        "w3 = \"bad\"\n",
        "w1_w2_dist = wv_from_bin.distance(w1, w2)\n",
        "w1_w3_dist = wv_from_bin.distance(w1, w3)\n",
        "\n",
        "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\n",
        "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))\n",
        "\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZV7c7_4s9Is"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**Example found**: w1=\"good\", w2=\"excellent\" (synonyms), w3=\"bad\" (antonym)\n",
        "\n",
        "In this case, \"good\" is closer to its antonym \"bad\" than to its synonym \"excellent\".\n",
        "\n",
        "**Possible explanations**:\n",
        "\n",
        "1. **Distributional Hypothesis and Context Similarity**: Word2Vec learns embeddings based on the distributional hypothesis - words that appear in similar contexts have similar meanings. The key insight is that \"good\" and \"bad\" often appear in **very similar contexts** because they are used to evaluate the same things:\n",
        "   - \"The movie was good/bad\"\n",
        "   - \"That's a good/bad idea\"\n",
        "   - \"good/bad weather\"\n",
        "   \n",
        "   They are both common evaluative adjectives that modify similar nouns.\n",
        "\n",
        "2. **Syntactic Similarity**: Antonyms often have the same part of speech and grammatical function. \"Good\" and \"bad\" are both simple, common adjectives. \"Excellent\", while also an adjective, is more formal and less frequently used, so it appears in somewhat different contexts.\n",
        "\n",
        "3. **Frequency and Register**: \"Good\" and \"bad\" are both extremely high-frequency words in casual language (like Twitter). \"Excellent\" is less common and more formal, so it appears in different types of discourse.\n",
        "\n",
        "4. **Semantic Opposition Creates Context Overlap**: Words that are semantically opposite are often used in contrasts or comparisons (\"not bad, actually good\"), creating strong co-occurrence patterns and shared contexts.\n",
        "\n",
        "5. **Embedding Space Geometry**: In the embedding space, semantic dimensions like \"quality evaluation\" bring antonyms close together, while differences in formality, frequency, or intensity may push synonyms apart.\n",
        "\n",
        "This counter-intuitive result highlights that word embeddings capture **contextual similarity** rather than pure **semantic similarity**. Antonyms, despite opposite meanings, can be more contextually similar than some synonyms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4G2X-TTpzwz"
      },
      "source": [
        "## Task 3: Utilize Word Embeddings\n",
        "\n",
        "Guess, you've seen such pictures already:  \n",
        "\n",
        "![Embeddings Relations](https://www.tensorflow.org/images/linear-relationships.png)\n",
        "*Source: [Tensorflow tutorial on Vector Representations of Words](https://www.tensorflow.org/tutorials/representation/word2vec)*\n",
        "\n",
        "In the first image, we observe the intricate relationships encoded within the word embeddings space. This encompasses various dimensions like gender differences (male-female) or verb tenses.\n",
        "\n",
        "**Interactive Exploration**\n",
        "\n",
        "To delve deeper and interactively explore these relationships, check out these resources:\n",
        "- [Word Vector Demo](http://bionlp-www.utu.fi/wv_demo/)\n",
        "- [Word2Viz](https://lamyiowce.github.io/word2viz/)\n",
        "\n",
        "These tools offer a playful yet insightful experience, allowing you to grasp the nuances and capabilities of word embeddings.\n",
        "\n",
        "**Our task point**\n",
        "\n",
        "Our focus will be on utilizing [gensim](https://radimrehurek.com/gensim/), a well-regarded Python library for word embeddings. Gensim makes it effortless to work with and leverage the power of word embeddings in various applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIvBhh71WIeS"
      },
      "source": [
        "### **3.1 Use Pretrained Embeddings**\n",
        "Base on gensim, we can easily use a well-pretrained embeddings model. There are a number of such models in <font color=\"blue\">gensim</font>, you can call `api.info()` to get the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEazfh1s9eki"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load('glove-twitter-25')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPQxqjIGZxt_"
      },
      "source": [
        "**use word embedidngs with gensim**\n",
        "\n",
        "Yay, we have loaded well-built word embedings models, now let's learn how to use it.\n",
        "\n",
        "1. To get word's vector, well, call `get_vector`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uF6iF6A9uGQ"
      },
      "outputs": [],
      "source": [
        "model.get_vector('anything')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiXwAZTsaHCf"
      },
      "source": [
        "2. To get most similar words for the given one :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57uH83XZaI6p"
      },
      "outputs": [],
      "source": [
        "model.most_similar('bread')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtyp__uQaVcR"
      },
      "source": [
        "3. Analogies with word embeddings\n",
        "\n",
        "It can do such magic (`woman` + `grandfather` - `man`) :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9igEyCm6aqfU"
      },
      "outputs": [],
      "source": [
        "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
        "model.most_similar(positive=['woman', 'grandfather'], negative=['man'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwCkJSNraruT"
      },
      "source": [
        "And this too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4t94HZXa1vd"
      },
      "outputs": [],
      "source": [
        "model.most_similar([model.get_vector('coder') - model.get_vector('brain') + model.get_vector('money')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c23cwWhKvtXp"
      },
      "source": [
        "That is, who is like coder, with money and without brains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a15dwaha36x"
      },
      "source": [
        "**<font color=\"red\">[Task]</font>** : Run an interesting analogy example\n",
        "\n",
        "**Hint**: Similar to (`woman` + `grandfather` - `man`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0D04rXsa_al"
      },
      "outputs": [],
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "# Example: king - man + woman = queen\n",
        "model.most_similar(positive=['king', 'woman'], negative=['man'])\n",
        "\n",
        "# Another example: paris - france + italy = rome\n",
        "model.most_similar(positive=['paris', 'italy'], negative=['france'])\n",
        "\n",
        "# Creative example: doctor - hospital + school = teacher\n",
        "model.most_similar(positive=['doctor', 'school'], negative=['hospital'])\n",
        "\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szS69OEObDzy"
      },
      "source": [
        "### **3.2 Finding the Most Similar Sentence**\n",
        "\n",
        "In this section, we present a method for sentence retrieval based on word embeddings.\n",
        "\n",
        "The key point is to construct *sentence embeddings*. The simplest method to obtain a sentence embedding is by averaging the embeddings of the words within the sentence.\n",
        "\n",
        "*You are probably thinking, 'What a dumb idea, why on earth the average of embedding should contain any useful information'. Well, check [this paper](https://arxiv.org/pdf/1805.09843.pdf).*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPJdCsjtfDxJ"
      },
      "source": [
        "1. Get Sentence Embedding\n",
        "\n",
        "**<font color=\"red\">[Task]</font>** : Implement a function to compute sentence embeddings.\n",
        "\n",
        "**Hint**: Tokenize and lowercase the texts. Calculate the mean embedding for words with known embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LPw1fRg1GaW"
      },
      "outputs": [],
      "source": [
        "def get_sentence_embedding(model, sentence):\n",
        "    \"\"\" Calcs sentence embedding as a mean of known word embeddings in the sentence.\n",
        "    If all the words are unknown, returns zero vector.\n",
        "    :param model: KeyedVectors instance\n",
        "    :param sentence: str or list of str (tokenized text)\n",
        "    \"\"\"\n",
        "    embedding = np.zeros([model.vector_size], dtype='float32')\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        words = word_tokenize(sentence.lower())\n",
        "    else:\n",
        "        words = sentence\n",
        "\n",
        "    sum_embedding = np.zeros([model.vector_size], dtype='float32')\n",
        "    words_in_model = 0\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    for word in words:\n",
        "        if word in model:\n",
        "            sum_embedding += model.get_vector(word)\n",
        "            words_in_model += 1\n",
        "    \n",
        "    if words_in_model > 0:\n",
        "        embedding = sum_embedding / words_in_model\n",
        "    \n",
        "    return embedding\n",
        "    # ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FbZbKPI1OZf"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5phpDHEcdK9"
      },
      "outputs": [],
      "source": [
        "vector = get_sentence_embedding(model, \"I'm very sure. This never happened to me before...\")\n",
        "assert vector.shape == (model.vector_size,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-8wclbCdTfw"
      },
      "source": [
        "2. **Building the Index**\n",
        "\n",
        "With our method ready, we can now embed all sentences in our corpus for retrieval purposes. In this case, we use data from Quora, sampling 1000 entries randomly, and converting them into sentence embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6B2c-bJdCrB"
      },
      "outputs": [],
      "source": [
        "quora_data = pd.read_csv('train.csv')\n",
        "corpus = list(quora_data.sample(1000)[['question1']].question1.replace(np.nan, '', regex=True).unique())\n",
        "text_vectors = np.array([get_sentence_embedding(model, sentence) for sentence in corpus])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXrJ-R8leSWR"
      },
      "outputs": [],
      "source": [
        "corpus[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiMtj5dXebbY"
      },
      "source": [
        "3. **Search**\n",
        "\n",
        "Now we are able perform search of the nearest neighbours to the given sentences in our base!\n",
        "\n",
        "\n",
        "We'll use cosine similarity of two vectors:\n",
        "$$\\text{cosine_similarity}(x, y) = \\frac{x^{T} y}{||x||\\cdot ||y||}$$\n",
        "\n",
        "*It's not a [distance](https://www.encyclopediaofmath.org/index.php/Metric) strictly speaking but we still can use it to search for the sentence vectors.*\n",
        "\n",
        "**<font color=\"red\">[Task]</font>** : IImplement the following function.\n",
        "\n",
        "**Hint:** Calc the similarity between `query` embedding and `text_vectors` using `cosine_similarity` function. Find `k` vectors with highest scores and return corresponding texts from `texts` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynEJW6E7eg0c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_nearest(model, text_vectors, texts, query, k=10):\n",
        "    query_vec = get_sentence_embedding(model, query)\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "    # Reshape query vector for cosine_similarity\n",
        "    query_vec = query_vec.reshape(1, -1)\n",
        "    \n",
        "    # Calculate cosine similarity between query and all text vectors\n",
        "    similarities = cosine_similarity(query_vec, text_vectors)[0]\n",
        "    \n",
        "    # Get indices of top k most similar texts\n",
        "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
        "    \n",
        "    # Return the corresponding texts\n",
        "    return [texts[i] for i in top_k_indices]\n",
        "\n",
        "    # ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is8SoYmHkQo5"
      },
      "source": [
        "Check it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49s4zB1OjXd4"
      },
      "outputs": [],
      "source": [
        "find_nearest(model, text_vectors, corpus, \"What's your biggest regret in life?\", k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA5CwGV8jU5_"
      },
      "source": [
        "### **Bias of Word Embeddings**\n",
        "\n",
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdIUrmnJxGvL"
      },
      "source": [
        "Here's an example showing word embeddings biases on gender:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWngJZCWxduU",
        "outputId": "661978c9-f2ee-4de1-ac10-935593d60c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('maths', 0.7983574867248535), ('basis', 0.7973601222038269), ('humør', 0.7948898673057556), ('cert', 0.7902684807777405), ('mulig', 0.7874146699905396), ('spændende', 0.7728654742240906), ('dårligt', 0.7700908184051514), ('latter', 0.7676339745521545), ('noget', 0.7676041126251221), ('vet', 0.7675378918647766)]\n",
            "\n",
            "[('representation', 0.871566116809845), ('encourages', 0.8626720309257507), ('empowering', 0.8612703084945679), ('intellectual', 0.8564386963844299), ('influences', 0.8559868931770325), ('ethical', 0.8550471663475037), ('affairs', 0.8541139960289001), ('behaviors', 0.8481355905532837), ('advocacy', 0.8439522981643677), ('critic', 0.8406822085380554)]\n"
          ]
        }
      ],
      "source": [
        "print(model.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
        "print()\n",
        "print(model.most_similar(positive=['woman', 'profession'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpFDY9BByDBj"
      },
      "source": [
        "**<font color=\"red\">[Task]</font>** Identify an example of bias.\n",
        "\n",
        "**Hint:** Consider providing an example from perspectives such as race or sexual orientation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np_RQalnx0Bb"
      },
      "outputs": [],
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "# Example 1: Gender bias\n",
        "print(\"Gender bias example:\")\n",
        "print(model.most_similar(positive=['doctor', 'woman'], negative=['man']))\n",
        "print()\n",
        "print(model.most_similar(positive=['nurse', 'man'], negative=['woman']))\n",
        "print()\n",
        "\n",
        "# Example 2: Race/ethnicity bias\n",
        "print(\"Race/ethnicity related:\")\n",
        "print(model.most_similar(positive=['programmer', 'asian'], negative=['white']))\n",
        "print()\n",
        "\n",
        "# Example 3: Profession and gender\n",
        "print(\"Profession stereotypes:\")\n",
        "print(\"Engineer + female - male:\")\n",
        "print(model.most_similar(positive=['engineer', 'female'], negative=['male']))\n",
        "print()\n",
        "print(\"Secretary + male - female:\")\n",
        "print(model.most_similar(positive=['secretary', 'male'], negative=['female']))\n",
        "\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "902RJydFyjMx"
      },
      "source": [
        "**<font color=\"red\">[Task]</font>** Thinking About Bias.\n",
        "\n",
        "**Hint:** Briefly explain how bias can be introduced into word embeddings and suggest one method to mitigate these biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8zWOHg3065B"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "## How Bias is Introduced into Word Embeddings\n",
        "\n",
        "Word embeddings inherit biases from their training data through several mechanisms:\n",
        "\n",
        "1. **Statistical Patterns in Training Data**: Word2Vec and similar models learn from the statistical co-occurrence patterns in text. If the training corpus (e.g., news articles, books, social media) reflects societal biases, these biases become encoded in the embeddings. For example:\n",
        "   - If \"doctor\" appears more frequently with male pronouns and \"nurse\" with female pronouns in the corpus, the embeddings will capture this gender association\n",
        "   - Historical texts may contain outdated stereotypes about race, profession, or social roles\n",
        "\n",
        "2. **Societal and Cultural Biases**: The text corpora reflect real-world biases present in society when the text was written. This includes:\n",
        "   - Gender stereotypes (e.g., women associated with domestic roles, men with technical professions)\n",
        "   - Racial stereotypes and discrimination\n",
        "   - Socioeconomic biases\n",
        "   - Age-related biases\n",
        "\n",
        "3. **Corpus Selection Bias**: The choice of training data matters. If the corpus is not diverse or representative (e.g., predominantly male authors, Western perspectives, specific time periods), the embeddings will reflect these limitations.\n",
        "\n",
        "4. **Reporting Bias**: News and social media may over-represent certain associations or stereotypes, even if they don't reflect reality accurately.\n",
        "\n",
        "## Method to Mitigate These Biases\n",
        "\n",
        "**Debiasing through Post-Processing (Bolukbasi et al., 2016 method)**:\n",
        "\n",
        "One effective approach is to identify and neutralize bias dimensions in the embedding space:\n",
        "\n",
        "1. **Identify the bias subspace**: \n",
        "   - Define sets of word pairs that represent the bias (e.g., he/she, man/woman for gender)\n",
        "   - Use these pairs to identify the direction in the embedding space that represents this bias\n",
        "\n",
        "2. **Neutralize and equalize**:\n",
        "   - For gender-neutral words (like \"doctor\", \"engineer\"), remove the component of their embeddings that lies in the bias subspace (neutralization)\n",
        "   - For gender-specific word pairs (like \"actor/actress\"), ensure they are equidistant from neutral words (equalization)\n",
        "\n",
        "3. **Alternative methods**:\n",
        "   - **Data augmentation**: Add counterfactual examples to training data (e.g., \"female engineer\", \"male nurse\")\n",
        "   - **Adversarial debiasing**: Train the model with an adversarial component that penalizes the model for learning biased representations\n",
        "   - **Contextualized embeddings**: Use models like BERT that generate context-dependent embeddings, which can reduce (but not eliminate) bias\n",
        "   - **Diverse training data**: Ensure training corpora include diverse perspectives and deliberately include counter-stereotypical examples\n",
        "\n",
        "**Important Note**: Complete debiasing is challenging, and trade-offs exist between debiasing and maintaining embedding quality for downstream tasks. Ongoing research continues to develop better methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqDmuu7m_PB5"
      },
      "source": [
        "## Supplementary Materials\n",
        "Source from [DeepNLP-Course of DanAnastasyev](https://colab.research.google.com/drive/1o65wrq6RYgWyyMvNP8r9ZknXBniDoXrn#forceEdit=true&offline=true&sandboxMode=true)\n",
        "\n",
        "## To read\n",
        "### Blogs\n",
        "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
        "[On word embeddings - Part 2: Approximating the Softmax, Sebastian Ruder](http://ruder.io/word-embeddings-softmax/index.html)  \n",
        "[Word2Vec Tutorial - The Skip-Gram Model, Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  \n",
        "[Word2Vec Tutorial Part 2 - Negative Sampling, Chris McCormick](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
        "\n",
        "### Papers\n",
        "[Word2vec Parameter Learning Explained (2014), Xin Rong](https://arxiv.org/abs/1411.2738)  \n",
        "[Neural word embedding as implicit matrix factorization (2014), Levy, Omer, and Yoav Goldberg](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)  \n",
        "\n",
        "### Enhancing Embeddings\n",
        "[Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf)  \n",
        "[Not All Neural Embeddings are Born Equal (2014)](https://arxiv.org/pdf/1410.0718.pdf)  \n",
        "[Retrofitting Word Vectors to Semantic Lexicons (2014), M. Faruqui, et al.](https://arxiv.org/pdf/1411.4166.pdf)  \n",
        "[All-but-the-top: Simple and Effective Postprocessing for Word Representations (2017), Mu, et al.](https://arxiv.org/pdf/1702.01417.pdf)  \n",
        "\n",
        "### Sentence Embeddings\n",
        "[Skip-Thought Vectors (2015), Kiros, et al.](https://arxiv.org/pdf/1506.06726)  \n",
        "\n",
        "### Backpropagation\n",
        "[Backpropagation, Intuitions, cs231n + next parts in the Module 1](http://cs231n.github.io/optimization-2/)   \n",
        "[Calculus on Computational Graphs: Backpropagation, Christopher Olah](http://colah.github.io/posts/2015-08-Backprop/)\n",
        "\n",
        "## To watch\n",
        "[cs224n \"Lecture 2 - Word Vector Representations: word2vec\"](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)  \n",
        "[cs224n \"Lecture 5 - Backpropagation\"](https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ_O-O0wkool"
      },
      "source": [
        "\n",
        "## Acknowledgement\n",
        "\n",
        "This assignment was developed with reference to the following course materials:\n",
        "- [DeepNLP Course by Dan Anastasyev](https://github.com/DanAnastasyev/DeepNLP-Course?tab=readme-ov-file)\n",
        "- [Exploring Word Vectors from Stanford's CS224N](https://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring_word_vectors.html)\n",
        "- [Natural Language Processing course from Princeton University](https://nlp.cs.princeton.edu/cos484-sp21/)\n",
        "- [Yandex Data School NLP Course Week 1 Seminar](https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/2023/week01_embeddings/seminar.ipynb#scrollTo=9m7GZWVk-jrW)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}