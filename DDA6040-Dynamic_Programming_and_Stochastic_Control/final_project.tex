\documentclass{article} % For LaTeX2e
\usepackage[preprint]{../../template/neurips2025/neurips_2025}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{../../template/neurips2025/math_commands.tex}

% \usepackage{neurips_2025} % Uncomment for camera-ready version, but NOT for submission.

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}        % math environments and commands
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{lipsum}         % lorem ipsum text


\title{Transformers as Dynamic Programming Solvers\thanks{This manuscript serves as the final project for the course DDA6040.}}

\author{Zhenrui Zheng$^\dagger$ \& Zhixuan Tan\thanks{Equal contribution} \vspace{0.5em} \\
\small{School of Data Science} \\
\small{Chinese University of Hong Kong, Shenzhen} \\
\texttt{\{225040512, 225040506\}@link.cuhk.edu.cn}
}

\begin{document}

\maketitle


\begin{abstract}
Large Transformer models exhibit a remarkable capacity for in-context few-shot learning, enabling them to generalize to novel tasks from a few demonstrative examples. Despite extensive research into this phenomenon, its application to emulating structured algorithmic paradigms, particularly dynamic programming (DP), remains underexplored. This work provides empirical evidence that Transformers can function as in-context solvers for problems requiring dynamic programming. We specifically investigate this capability through the lens of the single-source shortest path (SSSP) problem on directed graphs. Our findings indicate that, beyond this specific case, Transformers can learn to effectively implement the recursive logic inherent in DP algorithms, suggesting their potential as general-purpose algorithmic executors for this problem class. Furthermore, we conduct a series of analyses to dissect the factors influencing this emergent capability, with a focus on training data characteristics. A key finding reveals a strong positive correlation between the model's generalization performance and both the size of the training dataset and the diversity of node label mappings; larger, more diverse training data significantly enhances the model's capacity for in-context algorithmic execution. However, we also observe substantial variability in generalization performance across different random initializations, indicating that the optimization landscape contains multiple local minima with varying generalization properties.
\end{abstract}

\section{Introduction}

The advent of large language models (LLMs) based on the Transformer architecture has introduced a new paradigm of computation: in-context learning (ICL). This emergent capability allows models to rapidly adapt to novel tasks by conditioning on a few demonstrative examples instantiated within the input prompt. A significant line of inquiry has focused on elucidating the underlying mechanisms of ICL, revealing that Transformers can, in certain contexts, approximate computational processes such as learning linear functions and implementing iterative optimization algorithms like gradient descent.

Despite these advances, the capacity of Transformers to emulate more complex, state-dependent algorithmic paradigms remains an open question. Dynamic programming (DP) stands as a cornerstone of algorithm design, offering efficient solutions to a wide array of optimization problems by adhering to the principle of optimality and leveraging memoization of solutions to overlapping subproblems. The recursive and stateful nature of DP presents a formidable challenge for standard Transformer architectures, whose core mechanism is not explicitly designed for maintaining structured memory or executing recursive logic. Consequently, whether Transformers can internalize and execute the core principles of DP is a critical, yet underexplored, research frontier.

This paper aims to bridge this gap by systematically investigating the ability of Transformers to function as in-context dynamic programming solvers. We select the single-source shortest path (SSSP) problem on directed graphs—a canonical exemplar of DP—as our primary testbed. The problem is framed as an autoregressive sequence generation task, where the model is trained to produce the step-by-step execution trace of a DP-based algorithm (e.g., a variant of Bellman-Ford or Dijkstra). Our empirical results provide compelling evidence that Transformers can indeed learn to approximate the iterative state updates and decision-making processes inherent to the DP algorithm, successfully solving SSSP instances not seen during training.

Furthermore, we conduct a detailed analysis to delineate the factors conditioning this emergent algorithmic capability. A central finding of our investigation is the pivotal role of data characteristics in enabling generalization. We demonstrate that the model's generalization performance on in-context DP tasks is critically dependent on both the size of the training dataset and the diversity of node label mappings used during training. Models exposed to larger, more diverse training data exhibit significantly enhanced performance, suggesting that increased diversity compels the model to learn the abstract, symbolic structure of the algorithm rather than relying on superficial correlations in the input sequences. However, we also observe substantial variability in generalization performance across different random initializations, even under identical training conditions, indicating that the optimization landscape contains multiple local minima with varying generalization properties. This insight underscores the importance of data curation and training strategies in unlocking the latent algorithmic reasoning abilities of large language models.

\section{Background and Related Work}

\subsection{In-Context Learning of Algorithms}

The capability of Transformers to learn functions in-context has been a major focus of recent theoretical machine learning research.

\begin{itemize}
    \item \textbf{Linear Function Learning:} Garg et al. (2022) and Akyürek et al. (2022) established that Transformers can learn linear regression in-context, effectively implementing OLS or Gradient Descent. This laid the groundwork for viewing ICL as ``algorithmic.''
    \item \textbf{Complexity Hierarchy:} Bhattamishra et al. (2023) explored the complexity classes Transformers can handle, showing they can learn simple Boolean functions but struggle with complex automata without CoT.
    \item \textbf{Algorithmic Emulation:} Hu et al. (2025) proved that fixed-weight Transformers can emulate a broad class of algorithms via prompting, effectively acting as ``prompt-programmable'' machines.
    \item \textbf{In-Context TD Learning:} Wang et al. (2024) demonstrated that Transformers can perform Temporal Difference learning in-context, a direct link to the Bellman updates used in DP.
\end{itemize}

\subsection{Chain-of-Thought (CoT) and ``Scratchpads''}

For DP problems, which require storing intermediate states, standard ICL (predicting the answer directly) is often insufficient.

\begin{itemize}
    \item \textbf{CoT for DP:} Cheng et al. (2023) and Zhou et al. (2023) theoretically and empirically showed that CoT enables Transformers to solve DP problems like Longest Increasing Subsequence (LIS) and Edit Distance. The CoT effectively ``unrolls'' the DP table into the context window.
    \item \textbf{Scratchpads:} Nye et al. (2021) introduced the ``scratchpad'' concept, allowing the model to write out intermediate computation steps. This is crucial for SSSP on large graphs, as the path finding requires transitive steps.
    \item \textbf{Recursion of Thought:} Lee et al. (2023) extended this to ``Recursion of Thought,'' using a divide-and-conquer strategy to solve problems too large for a single context.
    \item \textbf{Transformer Learning $A^*$ Algorithm:} Lehnert et al. (2024) showed that Transformers can learn the $A^*$ algorithm in grid world pathfinding and Sokoban problems, showing the potential of Transformers to learn complex algorithms.
\end{itemize}

\subsection{Neural Algorithmic Reasoning (NAR)}

\begin{itemize}
    \item \textbf{DeepMind's CLRS:} Veličković et al. (2022) established the CLRS-30 benchmark, pushing for models that align with classical algorithms (Bellman-Ford, Prim's, etc.). They emphasize ``Algorithmic Alignment''—the architecture should match the algorithm.
    \item \textbf{Multiple Solutions:} Georgiev et al. (2024) explored NAR for problems with multiple correct solutions (like SSSP on unweighted graphs), arguing that models should learn the distribution of solutions.
    \item \textbf{Knapsack \& Pseudo-Polynomial:} Pozgaj et al. (2025) extended NAR to pseudo-polynomial problems like Knapsack, using DP table supervision.
\end{itemize}

\section{Methodology}

\subsection{Problem Formulation: Single-Source Shortest Path}

We focus on the single-source shortest path (SSSP) problem on directed graphs as our testbed for evaluating Transformers as dynamic programming solvers. Formally, given a directed graph $G = (V, E)$ with vertex set $V$ and edge set $E$, where each edge $(u, v) \in E$ has a non-negative weight $w(u, v) \geq 0$, and a source vertex $s \in V$ and sink vertex $g \in V$, the problem is to find the shortest path from $s$ to $g$.

This problem can be solved using dynamic programming through Dijkstra's algorithm, which maintains a priority queue and iteratively relaxes edges. The DP formulation for SSSP can be expressed as:
\begin{align}
    d(s) &= 0, \\
    d(v) &= \min_{(u,v) \in E} \{d(u) + w(u,v)\} \quad \forall v \in V \setminus \{s\},
\end{align}
where $d(v)$ represents the shortest distance from source $s$ to vertex $v$. We use graphs with a fixed size of $|V| = 8$ vertices, and edge weights are sampled from the range $[0, 9]$.

\subsection{Formulating Shortest Path as Next Token Prediction Problem}

To enable the Transformer to learn and execute the Dijkstra algorithm, we formulate the shortest path problem as a next token prediction task. Each problem instance is encoded as a token sequence consisting of two parts: a \textit{prompt} that describes the graph structure and query, and an \textit{events} sequence that represents the step-by-step execution of the Dijkstra algorithm.

\textbf{Prompt Encoding:} The prompt encodes the graph structure and query as a sequence of tokens:
\begin{align}
    \text{prompt} = [\text{start}, \text{node}_s, \text{sink}, \text{node}_g, \text{edge}, \text{node}_i, \text{node}_j, w_{ij}, \ldots],
\end{align}
where $\text{node}_s$ and $\text{node}_g$ denote the source and sink vertices, and each edge $(i, j)$ with weight $w_{ij}$ is represented by the tokens $[\text{edge}, \text{node}_i, \text{node}_j, w_{ij}]$.

\textbf{Events Encoding:} The events sequence captures the execution trace of Dijkstra's algorithm, representing the algorithm's state transitions:
\begin{align}
    \text{events} = [\text{bos}, \text{close}, \text{node}_v, d_v, \text{open}, \text{node}_u, d_u, \ldots, \text{path}, \text{node}_v, d_v, \ldots, \text{eos}],
\end{align}
where $\text{close}$ events mark vertices being extracted from the priority queue with their final distances, $\text{open}$ events mark vertices being added or updated in the queue, and $\text{path}$ events trace the shortest path from sink to source\footnotemark[1] by backtracking.
\footnotetext[1]{When reconstructing the shortest path, we output the path in a backward manner, from the sink to the source. This is because if we output the path from the source to the sink, finding the next node starting from the source (given the pathfinding process) would have a linear time complexity, which is computationally impossible for transformers (with constant computation). Nevertheless, in practice, we found that both methods work, but we still adopt the more reasonable approach.}

\textbf{Training Format:} During training, the model receives the concatenated sequence $\text{prompt} \oplus \text{events}$ as input, where $\oplus$ denotes concatenation. The model is trained to predict the events sequence autoregressively, with loss computed only on the events portion (the prompt portion is masked out in the loss computation). This design allows the model to learn the mapping from graph structure to algorithmic execution trace.

\section{Experiments}

\subsection{Experimental Setup}

We employ a decoder-only Transformer architecture based on the LLaMA model family. The vocabulary consists of special tokens, numerical tokens (0-99), and node label tokens. Node labels are randomly reassigned to different token IDs for each training sample, with the reassignment range controlled by a hyperparameter determining label diversity. During evaluation, node labels are sampled from a held-out label set unseen during training.

We generate synthetic directed graphs by sampling edge probability $p_{\text{edge}}$ uniformly from $[0, 1]$ and including directed edges with probability $p_{\text{edge}}$. Edge weights are sampled uniformly from integers in $[0, 9]$. Source and sink vertices are randomly selected, and the Dijkstra algorithm generates ground-truth execution traces. The model is trained using standard next-token prediction with cross-entropy loss, with loss computed only on the events portion.

\subsection{Experimental Design}

We systematically vary two data characteristics: (1) training dataset size (1,000, 10,000, 100,000 samples), and (2) node label diversity (10, 100, 1,000 distinct mappings), yielding nine configurations. For each configuration, we maintain a fixed total number of training steps and repeat with four random seeds, resulting in 36 training runs.

Throughout training, we monitor: (1) \textit{training loss} (cross-entropy on training set), (2) \textit{success rate} (fraction of test instances with valid shortest path solutions), and (3) \textit{perplexity (PPL)} (evaluated on held-out test set using teacher-forcing). The evaluation dataset is 1,000 samples held out from training data.

\subsection{Results}

Our results demonstrate that Transformers can learn to solve SSSP problems, with performance varying significantly across configurations and random initializations. While all models achieve low training loss, generalization to held-out test instances reveals substantial variation.

Figure~\ref{fig:training_curves} shows training dynamics across configurations. Models that successfully learn generalizable algorithms show steady increases in test success rate alongside decreasing test perplexity, whereas models that fail to generalize maintain high test perplexity and low success rates despite low training loss.

\subsubsection{Stochasticity and Generalization}

A striking finding is significant variability in generalization performance across random seeds under identical conditions. For a given configuration, different initializations lead to dramatically different outcomes: some seeds achieve near-perfect test performance, while others fail to generalize beyond memorized patterns.

Figure~\ref{fig:seed_variability} illustrates this phenomenon. With dataset size 1,000 and label diversity 10, test success rates across four seeds range from approximately 15\% to 85\%, indicating high sensitivity to initialization. This suggests the optimization landscape contains multiple local minima with varying generalization properties.

\subsubsection{Impact of Data Characteristics}

Figure~\ref{fig:data_characteristics} summarizes the relationship between data characteristics and average test success rate. We observe a strong positive correlation between dataset size and generalization performance. Models trained on 100,000 samples consistently outperform those trained on smaller datasets, with larger datasets also reducing variance across random seeds.

Similarly, increasing label diversity improves generalization. Models trained with 1,000 distinct mappings consistently outperform those with 10 or 100 mappings across all dataset sizes. Limited diversity allows models to learn associations between specific node labels and algorithmic steps, which break down on held-out label sets.

The benefits are complementary: the configuration with 100,000 samples and 1,000 label mappings achieves the highest and most consistent performance, with all four seeds achieving test success rates above 90\%. Conversely, the configuration with 1,000 samples and 10 label mappings shows the highest variability and lowest average performance.

\section{Discussion}

Our findings have several implications for developing general-purpose DP solvers. The observed sensitivity to random initialization suggests training strategies should incorporate multiple random seeds and potentially ensemble methods. The strong correlation between data diversity and generalization performance highlights the critical importance of data curation: simply scaling up training data is insufficient if it lacks representational diversity.

The distinction between training convergence and generalization success indicates that training loss alone is inadequate for assessing whether a model has learned a generalizable algorithm. Evaluation must explicitly test generalization to held-out distributions. The success of Transformers in learning DP algorithms, despite lacking explicit mechanisms for structured memory or recursive logic, suggests the in-context learning paradigm may be more powerful than previously recognized.

Our investigation is limited to SSSP on graphs of fixed size (8 vertices). Extending to other DP problems (longest common subsequence, edit distance, knapsack) and larger graph sizes would strengthen our conclusions. Future work could investigate how architectural choices interact with data characteristics, analyze the optimization landscape to understand why certain seeds lead to generalizable solutions, and explore more efficient training strategies.

\section{Conclusion}

This work demonstrates that Transformers can function as in-context solvers for dynamic programming problems, specifically learning and executing the single-source shortest path algorithm. Our investigation reveals that generalization is highly sensitive to both training data characteristics and random initialization.

A central finding is the critical role of data diversity: both dataset size and node label diversity significantly influence the likelihood of learning generalizable algorithmic patterns. We also observe substantial variability across random seeds, suggesting the optimization landscape contains multiple local minima with varying generalization properties.

These findings underscore the importance of careful data curation and robust evaluation protocols that explicitly test generalization. While challenges remain, our results suggest Transformers may hold promise as general-purpose executors of dynamic programming algorithms.

\appendix
\section{Appendix}
You may include other additional sections here.

\end{document}
